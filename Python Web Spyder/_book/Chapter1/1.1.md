# Robots协议
Robots协议（爬虫协议）的全称是“网络爬虫排除标准”，网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。该协议是国际互联网界通行的道德规发，每一个爬虫都应该遵守这项协议。

例如淘宝网的**robots.txt**(https://www.taobao.com/robots.txt)：
```
User-agent: Baiduspider #百度爬虫引擎
Allow: /article #允许访问/article.html、/article/12345.com
Allow: /oshtml
Allow: / wenzhang
Disallow: /product/ #禁止访问/product/12345.com
Disallow: / #禁止访问除Allow规定页面外的其他所有页面

User-Agent: Googlebot #谷歌爬虫引擎
Allow: /article
Allow: /oshtml
Allow: /product #允许访问/product.html、/product/12345.com
Allow: /spu
Allow: /dianpu
Allow: /wenzhang
Allow: /oversea
Disallow: /
```
在上面的robots文件中，淘宝网对用户代理为百度爬虫引擎进行了规定。

以**Allow**项的值开头的URL是允许robot访问的。例如，Allow: /article 允许百度爬虫引擎访问/article htm、/article/12345.com 等。

以**Disallow**项为开头的链接是不允许百度爬虫引擎访问的。例如，
Disallow: /product/ 不允许百度爬虫引擎访问/product/12345.com等。

最后一行，Disallow: /禁止百度爬虫访问除了 Allow 规定页面外的其他所有页面。
